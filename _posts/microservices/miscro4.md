---
title: Other - 服务治理架构价值-为什么要服务治理
date: 2021-04-01 17:29:11
categories: 
	- [Other]
tags:
  - http
  - glang
author: Jony
---


# 背景

公司目前要建立服务治理体系架构，建立服务化的之前先问下为什么需要服务治理？
可以先想象一下，随着公司的业务发展需要建立的服务将会越来越多，服务之间的依赖，关系将会越来越复杂
在这种错综复杂的调用如果没有一个能掌握全局的架构人员，可能任何一点的改动都会牵一发而动全身。
所以服务治理首先要解决**复杂性**问题。将服务与服务之间的依赖清晰化、调理化组件之间的上下文边界。


# 架构演进过程

根据背景大致了解了公司目前服务治理要解决的主要问题，所以先从服务架构的演进过程推演服务治理的演进过程。
服务架构从历史到目前的主流的体系架构，大致分为以下几个过程：

### 单一应用架构

单一应用架构也是企业初期最长采用的架构，网站流量较小、所需功能较少只需要一个应用，将所有的功能部署
在一起基本可以支持用户的访问量

### 垂直应用架构

当访问量增大、功能需求增多单一应用架构已经不能够支撑当前的访问量，这个时候会将功能不相干的业务独立
拆分出来建立多个系统提供应用。比如：MVC 模式前后端分离

### 分布式服务架构

当垂直应用、功能进一步扩大，应用和应用之间必定就会出现依赖关系，从业务架构体系上就会将共享的功能
抽离出来，细分为单独的服务对外提供访问。拆分出来独立服务经过时间的积累就会逐渐形成独立的稳定的服
务中心。服务与服务之间的拆分、服务整合就会形成 **分布式服务框架（RPC）**

### 流动计算架构

当服务越来越多，关于容量的评估：小服务资源的浪费、大服务资源的不足等一系列问题慢慢的就会凸显出来
这个时候就需要统一的调度中心，基于访问压力实时管理资源容量，提高资源利用率。为了提供资源利用率的
关键就会引入 **服务治理(SOA)**

### 网格化架构

当服务越来越多，使用服务治理的 SDK 越来越多，功能需求也就越来越多。随着老系统的沉淀和新系统的不
断更新，SDK 版本差异化将会越来越大，推动 SDK 升级将会越来越困。开发人员对 SDK 的更新慢慢的出现
了厌倦的情绪。Service Mesh 的出现完美解决了这个问题。将 SDK 功能轻薄化，降低 SDK 的升级改动
通过 Sidecar 的形式无感知的接入

# 服务治理过程

通过架构的演进过程，可以大致的了解到公司目前的架构处于哪个阶段。下面我们了解下整个服务治理的演进
过程和存在的必要性。
在大规模接入服务治理之前，应用与应用之间的依赖关系，只能通过简单的端口暴露和URL地址访问的形式进
行调用。通过一些 Nginx 或者 Iptables 是等组件负载均衡器接入 

### 1. 当服务越来越多，URL 也会随着变的庞大 ，管理起来非常困难，负载均衡的压力将会越来越大

此时就需要注册中心、动态的注册和发现服务，让服务实例的位置更加透明。消费方通过一定的手段获取到服
务方的实例列表，实现软负载均衡的方式，降低对负载均衡器的依赖，也可以降低一些运维成本并且提高一些
性能

### 2. 再推进一步，服务之间与服务之间的依赖越来越多，依赖关系变的错综复杂。可能架构师都已经不
###    能 Hold 住全局

这个时候，应该自动构出应用的之间依赖关系图帮助开发人员、运维人员、架构师等理解应用之间的依赖关系

### 3. 随着应用的调用量越来越大，服务的容量问题就暴露出来。比如双十一的流量突增需要
###    评估需要多少机器？什么时间段扩缩容？

为了准确的预估容量就需要获取每天的调用量、相应时间、资源利用率等数据都需要准确的统计出来，作为
扩缩容衡量的一个参考。

### 4. 随着应用数量扩大，沟通成本也会随着上升，调用某个功能是否有实现、服务的参数应该怎么定义、
###    某个服务失败应该找谁等一系列问题

对于上面这种情况就需要将每个服务提供的功能、参数定义、负责人建立档案以检索的方式提供给开发人员

### 5. 随着时间的推移，很多服务已经过了最热的生命周期，只有丁点的流量，但是却需要两台提供服务的
###    实例，如何减少资源的浪费


总上所述了应该也是目前我们可能会多多少少遇到的问题，所以我们需要通过服务治理架构方式来解决上
述遇到的问题。解决上述问题的时候就需要搭建一些基础设施和制定一些共同遵守的标准协议，来协助整个
服务治理的落地。

## 标准化的制定

- 服务的定义：提供方如何暴露一个服务，调用方应该如何去调用一个服务
- 服务的调用：服务的调用通常是远程，以何种方式调用？阻塞、非阻塞？
- 服务的通信协议：服务方与调用方如何标准化通信协议？二进制、JSON文本？
- 服务的版本化：服务肯定需要升级，升级的话是否需要版本化？是否需要向下兼容？
- 服务的安全：服务调用时需要加密？还是裸奔？

## 基础设施的落地

将架构演练过程和服务治理过程综合起来看我们需要下面这些基础设施支撑起整个服务治理的方案。
- 注册中心：服务注册与发现、软负载均衡与容错、服务的流量调度
- 服务监控：服务监控与统计、服务依赖关系、服务使用情况报告、服务健康检测、服务调用链跟踪
- 服务自动化测试：服务自动测试、服务压测平台
- 服务归属：服务分层架构、服务权限控制、服务负责人、服务文档

在公司内部方案落地需要哪些方面：

1. 注册中心的落地，公司目前的注册中心仅为 `DNS` ，`DNS` 优点在于跨语言、操作系统天然支持
   使用起来较为方便。也是公司目前最常用的方式。缺点也比较明显：
   - 需要手动维护 `hosts`，运维负责成本较高
   - 不稳定，每次请求之前都需要 `DNS` 解析，如果 `DNS` 出现以外故障不可服务，那么服务将无法发现
   - 只限制与 `k8s `接入，虚拟机、裸机其他平台无法接入
   - 提供功能有限，不利于扩展及定制化
    - 只能注册 `IP` 级节点信息，开发人员需要维护 `PORT` 
2. 服务监控的落地，通过服务端的信息上报或者客户端的信息上将实时的调用成功、失败等信息上报
   到监控平台，汇总成性能指标图形化展示
   - 目前通过 cat 或者 kibana
   - 通过上传 ES 将 metric 信息二次架构展示
3. 服务归属，统一建立服务管理平台，目前使用 dot 

在服务治理系统落地的过程中一些细节问题的思考，服务治理SDK 的开发只需要轻薄化提供必要的一些
功能即可，如：服务注册（k8s Controller 代替）、客户服务发现、简单服务调用。只需要精简化
最为底层基础保障和提供相互依赖时的一些基础信息即可。


# Service Mesh

认识一下 `Service Mesh` 的背景和作用。官方对 `Service Mesh` 的解释：

   服务网格（Service Mesh）是处理服务间通信的基础设施层。它负责构成现代云原生应用程序的
   复杂服务拓扑来可靠地交付请求。在实践中，Service Mesh 通常以轻量级网络代理阵列的形式
   实现，这些代理与应用程序代码部
   署在一起，对应用程序来说无需感知代理的存在。

`Service Mesh` 的核心价值：

- 微服务基础设施下沉： 微服务架构支撑、网络通信、治理等相关能力下沉到基础设施层，业务部门无
  需投入专人开发与维护
- 降低升级成本：Sidecar支持热升级，降低中间件和技术框架客户端、SDK升级成本
- 语言无关：提供多语言服务治理能力
- 降低复杂测试、演练成本：降低全链路压测、故障演练成本和业务侵入性

为何使用 `Service Mesh`？

`Service Mesh` 并没有给我们带来新功能，它是用于解决其他工具已经解决过的问题，只不过这次是在
以 Kubernetes 为基础的云原生生态环境下的实现。
传统模式下微服务为了提供完善生态会将更多的功能集成到 `SDK` 中，经过时间的积累就会形成胖客户端
的库，`Service Mesh` 是的出现不仅解决了众多的微服务之间的依赖和通讯复杂问题，还集成了`SDK`
中大多数功能，减少了 `SDK` 的职责。

# 关于公司内服务治理一些相关的问题

1\. **为什么有 `Service Mesh` 还要接入 `SDK`？**

从上面的资料就可以看出 `Service Mesh` 并不是为了替代 `SDK` 而是降低 `SDK` 职责，公司内
必定出现异构的情况，`Service Mesh` 就是为了避免多语言之间重复造轮子的情况，将比较重的功能
嫁接到 `Service Mesh` 中，由 `Service Mesh` 来完成。比如集群限流、流量调度等

2\. **为什么 `Service Mesh` 中有服务发现的功能还要在 `SDK` 中重复这个功能？**

`Service Mesh` 是作为 `Sidecar` 的方式与微服务框架一起启动，是两种不同的系统。而 
`Service Mesh` 也并不能保证 `100%` 出现宕机，而且 `Service Mesh` 作为独立的系统
本身也需要启动时间，并不能完全与服务同步。`SDK` 是集成到服务中随着服务同步启动，并可以
立刻提供服务，而且`SDK` 本身并不需要考虑宕机的情况，因为与服务拥有完全一致的生命周期，
所以 SDK 的接入是为了保证基本的生存能力。

3\. **为什么需要注册中心现有的 `hosts` 模式使用完全可以，接入注册中心感觉完全没有必要？**

首先注册中心的概念并不是因为微服务的存在而存在的，注册中心是系统拆分后必然出现的结果，注册
中心是收集了服务端多元化信息为服务调用后端某个实例作出最有的选择，接入注册中心后可见性的功能
就有：
- 服务可见性：客户端通过寻址就可以知道服务方的提供的服务地址和端口号
- 故障容错：客户端调用部分实例时发现异常调用异常，可以在极短的时间内将异常实例剔除，而不需要
  等待熔断，非常符合小流量上线，线上验错等一些场景
- 流量控制：服务方在下发实例时可以分配权重等基础信息，已确定某个实例承载更多的流量

而上面这些功能是 `hosts`  模式系统不了的，更为常见就是 `hosts` 模式只是提供 IP 而不能携带
端口号。另外 `hosts` 无法传递服务状态，就算出现的不健康的实例也无法剔除。

4\. **`Service Mesh` 好像没有接入注册中心了，还是觉得 `Service Mesh` 模式比较好**

**首先注册中心的概念并不是因为微服务**模式的存在而存在的，就 `Kubernetes` 而言本身就通过接
入 `Etcd` 为注册中心来完成实例的调度、部署等高可用操作。所以注册中心并不是为了微服务而产生的。
就传统的微服务以 `SDK` 的形式直接与注册中心发起交互，这是肉眼可见的。而 `Istio` 则是通过
`Pilot` 替代了边缘代理组件接入注册中心，`Pilot` 负责与注册中心交互获取实例、调度权重等通过
长连接的方式将指令下发到边缘代理组件中。

5\. **那现在是既要接入 `SDK` 还要接入 `Service Mesh` 吗？**

当然通过上面的分析就是可以知道接入 `SDK` 是为了保证最基本的生存能力，而接入 `Service Mesh`
是为了丰富轻薄化的 `SDK` ，微服务提供轻薄化 `SDK` 所没有的功能。根据目前业内的情况基本也都是
这种方式，比如字节跳动在开发微服务框架是就直接面向 `Servie Mesh` 编程。百度改造 `Sidecar`
通过心跳上报的方式判断当前实例下 `Sidecar` 的状态再判断服务是否需要接入。基本都是这两种情况。
而且就 `Service Mesh` 的组件 `Envoy` 是使用 `C++` 编写，本身接入就存在一定的技术高度，很
不利于公司的落地。目前公司主流的编程语言还是 `PHP`、`Java`、`Golang` 

6\. **纯接入 `Service Mesh` 有什么缺点，何时接入？**

存粹的接入 `Service Mesh` 不可避免的就是延迟，因为 `Sidecar` 模式是通过代理的模式在调用方
和服务方都有一级代理也就发生了两次跳转虽然耗时可能在毫秒级，但是如果对耗时高度敏感的业务接入 
`Service Mesh` 就要考虑是否能够接收延迟。当注册中心的数据基本完备和技术储备基本成熟的情况下
就会将符合要求的业务无声的接入到 `Service Mesh` 中

7\. `Service Mesh` 已经存在的注册中心了，为什么还要单独重新建立一个注册中心，而不是复用
    `Service Mesh` 的注册中心

8\. MySQL、外部依赖资源


nacos方案和 istio se路线（技术难度、当前异构适配程度、实施难度和工作量、核心业务支持的速度、
性能、拓展性、架构合理性、治理难度、未来趋势）

昨天我们讨论的内容和大方向，要落实到几个子任务，估计时间，有明确的预期（思路更新到Confluence上）了我们和梁田、晓腾同步一下；

为互联互通，
1、java怎么做；已经做了什么
2、php怎么做？
3、go怎么做？
4、k8s和k8s controller怎么做？
5、dot内如何支持，研发改动什么，运维变更什么？


nacos不是目的，只是一个途径（我们不会明确给青岛要求，你们来介入nacos吧）； 
而是， 我们要带着明确的目的（让devops实现具体的功能）来起草标准规范，然后准备前置的方案，
业务方用了这些方案，就能获得功能；已知的几个方向：比如：pod ip直通，还是可以方便的熔断，限流，监测+验活，调用链？

若不能整洁架构、习得思想、获得能力，却是换种语言，造一坨新鲜的屎吧
新生不怕，最惧成熟，那是固封陋习，不见天日，不嗤改进吧


落地方案价值







