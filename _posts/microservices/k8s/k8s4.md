---
title: Kubernetes - kube-proxy 无秘密（iptables）
date: 2021-10-18 16:42:00
categories: 
	- [gRPC]
tags:
  - microservices
  - Kubernetes
  - k8s
author: Jony
---

翻译：[Kubernetes Service Proxy](https://ssup2.github.io/theory_analysis/Kubernetes_Service_Proxy/)

# 背景

`kube-proxy` 是负责 k8s 集群内通信规则创建的组建，k8s 官方文档解释

> Kubernetes 网络代理在每个节点上运行。网络代理反映了每个节点上 Kubernetes API 中定义的服务，
> 并且可以执行简单的 TCP、UDP 和 SCTP 流转发，或者在一组后端进行 循环 TCP、UDP 和 SCTP 转发。

但是 kube-proxy 本身并不负责流量转发等工作。
Kubernetes 支持三种 Service Proxy 模式：iptables、IPVS 和 Userspace。根据服务代理方式分析服务请求报文路径。


# Service 和 Pod 信息

```bash
$ kubectl get pod -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP              NODE     NOMINATED NODE
my-nginx-756f645cd7-gh7sq         1/1     Running   14         15d   192.167.2.231   kube03   <none>
my-nginx-756f645cd7-hm7rg         1/1     Running   17         20d   192.167.2.206   kube03   <none>
my-nginx-756f645cd7-qfqbp         1/1     Running   16         20d   192.167.1.123   kube02   <none>

$ kubectl get pod -o wide
NAME                    TYPE           CLUSTER-IP       EXTERNAL-IP    PORT(S)                           AGE     SELECTOR
my-nginx-cluster        ClusterIP      10.103.1.234     <none>         80/TCP                            15d     run=my-nginx
my-nginx-loadbalancer   LoadBalancer   10.96.98.173     172.35.0.200   80:30781/TCP                      15d    run=my-nginx
my-nginx-nodeport       NodePort       10.97.229.148    <none>         80:30915/TCP 
```
这里展示了 `Kubernetes Service Proxy` 的 `Service` 和 `Pod` 信息。可以从上面的信息看出部署了三个 `nginx` Pod。并添加了
`ClusterIP` 类型的 `my-nginx-cluster` 、`NodePort` 类型的 `nginx-nodeport` 和 `LoadBalancer` 的 `my-nginx-loadbalancer`。

# Iptables 模式

![iptables模式下的服务请求包路径](/images/k8s/iptables_Mode_Service_Packet_Path.png)


`iptables` 模式下的 `KUBE-SERVICES`：

```bash
Chain KUBE-SERVICES (2 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 KUBE-MARK-MASQ  tcp  --  *      *      !192.167.0.0/16       10.96.98.173         /* default/my-nginx-loadbalancer: cluster IP */ tcp dpt:80
    0     0 KUBE-SVC-TNQCJ2KHUMKABQTD  tcp  --  *      *       0.0.0.0/0            10.96.98.173         /* default/my-nginx-loadbalancer: cluster IP */ tcp dpt:80
    0     0 KUBE-FW-TNQCJ2KHUMKABQTD  tcp  --  *      *       0.0.0.0/0            172.35.0.200         /* default/my-nginx-loadbalancer: loadbalancer IP */ tcp dpt:80
    0     0 KUBE-MARK-MASQ  tcp  --  *      *      !192.167.0.0/16       10.103.1.234         /* default/my-nginx-cluster: cluster IP */ tcp dpt:80
    0     0 KUBE-SVC-52FY5WPFTOHXARFK  tcp  --  *      *       0.0.0.0/0            10.103.1.234         /* default/my-nginx-cluster: cluster IP */ tcp dpt:80 
    0     0 KUBE-MARK-MASQ  tcp  --  *      *      !192.167.0.0/16       10.97.229.148        /* default/my-nginx-nodeport: cluster IP */ tcp dpt:80
    0     0 KUBE-SVC-6JXEEPSEELXY3JZG  tcp  --  *      *       0.0.0.0/0            10.97.229.148        /* default/my-nginx-nodeport: cluster IP */ tcp dpt:80
    0     0 KUBE-NODEPORTS  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL
```

`iptables` 模式下的 `KUBE-NODEPORTS`：

```bash
Chain KUBE-NODEPORTS (1 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 KUBE-MARK-MASQ  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/my-nginx-loadbalancer: */ tcp dpt:30781
    0     0 KUBE-SVC-TNQCJ2KHUMKABQTD  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/my-nginx-loadbalancer: */ tcp dpt:30781
    0     0 KUBE-MARK-MASQ  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/my-nginx-nodeport: */ tcp dpt:30915
    0     0 KUBE-SVC-6JXEEPSEELXY3JZG  tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/my-nginx-nodeport: */ tcp dpt:30915 
```

`iptables` 模式下的 `KUBE-FW-XXX`：

```bash
Chain KUBE-FW-TNQCJ2KHUMKABQTD (1 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 KUBE-MARK-MASQ  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/my-nginx-loadbalancer: loadbalancer IP */
    0     0 KUBE-SVC-TNQCJ2KHUMKABQTD  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/my-nginx-loadbalancer: loadbalancer IP */
    0     0 KUBE-MARK-DROP  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/my-nginx-loadbalancer: loadbalancer IP */
```

`iptables` 模式下的 `KUBE-SVC-XXX`：

```bash
Chain KUBE-SVC-TNQCJ2KHUMKABQTD (2 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 KUBE-SEP-6HM47TA5RTJFOZFJ  all  --  *      *       0.0.0.0/0            0.0.0.0/0            statistic mode random probability 0.33332999982
    0     0 KUBE-SEP-AHRDCNDYGFSFVA64  all  --  *      *       0.0.0.0/0            0.0.0.0/0            statistic mode random probability 0.50000000000
    0     0 KUBE-SEP-BK523K4AX5Y34OZL  all  --  *      *       0.0.0.0/0            0.0.0.0/0      
```

`iptables` 模式下的 `KUBE-SEP-XXX`：

```bash
Chain KUBE-SEP-6HM47TA5RTJFOZFJ (1 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 KUBE-MARK-MASQ  all  --  *      *       192.167.2.231        0.0.0.0/0
    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp to:192.167.2.231:80 
```

`iptables` 模式下的 `KUBE-POSTROUTING`：

```bash
Chain KUBE-POSTROUTING (1 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 MASQUERADE  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service traffic requiring SNAT */ mark match
0x4000/0x4000 
```

`iptables` 模式下的 `KUBE-MARK-MASQ`：

```bash
Chain KUBE-MARK-MASQ (23 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 MARK       all  --  *      *       0.0.0.0/0            0.0.0.0/0            MARK or 0x4000 
```

`iptables` 模式下的 `KUBE-MARK-DROP`：

```bash
Chain KUBE-MARK-DROP (10 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 MARK       all  --  *      *       0.0.0.0/0            0.0.0.0/0            MARK or 0x8000
```

这里的 `Service Proxy` 使用是 `iptables` 模式。这是 `Kubernetes` 当前使用的默认代理模式。
在上方图中展示了 `iptables` 模式下服务请求包的路径。上方的 `bash` 是展示了主要 `NAT` 表的内容。

由于大多数 `Pod` 传输的请求包是通过 `Pod` 的 `veth` 传递到主机的网络命名空间，因此请求包通过 `PREROUTING` 表传递到 `KUBE-SERVICES` 表。
`Pod` 或者 `Host`进程 使用 `Host` 的网络命名空间传输的请求数据包由 `OUTPUT` 表传递到 `KUBE-SERVICES` 表。

如果`KUBE-SERVICES` 表中请求包的目标`IP`和目标`Port`与`ClusterIP Service`的`IP`和`Port`匹配，则请求的包
被转发到`KUBE-SVC-XXX` 表，即`ClusterIP Service`的`NAT`表.

如果 `KUBE-SERVICES` 表中请求包的目标 `IP` 是节点自己的 `IP`，则将请求包转发到 `KUBE-NODEPORTS` 表。
如果 `KUBE-NODEPORTS` 表中请求报文的目标 `Port` 与 `NodePort Service` 的 `Port`匹配，则将请求报文传送到 `KUBE-SVC-XXX` 表，
即 `NodePort Service` 的 `NAT` 表。

如果`KUBE-SERVICES` 表中请求包的目标 `IP`和目标 `Port`与`LoadBalancer Service` 的`External IP`和`Port`匹配，
则将请求包转发到`KUBE-FW-XXX` 表，`LoadBalancer Service`的`NAT`表，然后再将 `LoadBalancer Service` 传送
到 `KUBE-SVC-XXX` 表，也就是 `NAT` 表

在`KUBE-SVC-XXX` 表中，请求包通过`iptables`的统计功能，在构成`Service`的`Pod`之间起到随机均匀负载均衡的作用。
在 *`KUBE-SVC-TNQCJ2KHUMKABQTD`* 中，由于`Service`由三个`pod`组成，可以看出请求包设置为随机均衡负载均衡，使用三个`KUBE-SEP-XXX`表。
在 `KUBE-SEP-XXX` 表中，请求包使用 `Pod` 的 `IP` 和 `Service` 中设置的 `Port` 进行 `DNAT`。
由`Pod`的`IP`发出的`DNAT`请求包通过`CNI Plugin`构建的容器网络传递给该`Pod`。

由于传递给`service`的请求包是通过`iptables`的`DNAT`传递给`Pod`的，所以`Pod`发出的响应包的`Src IP`应该`SNAT`到`Service IP`，
而不是`Pod IP` 。`iptables` 中未指定 `Serivce` 的 `SNAT` 规则。但是，`iptables`根据`Linux Kernel` 的`Conntrack`（连接跟踪）的 
`TCP` 连接信息对从 `Service Pod` 收到的响应数据包进行 `SNAT`。


# Source IP 

`Service`请求包的`Src IP`将被留存，或通过`Masquerade`作为`Host`的`IP`进行`SNAT`。
`KUBE-MARK-MASQ` 是一个表，将使用 `Masquerade` 对请求包进行标记。
`Marking`的`Packet`在`KUBE-POSTROUTING` 表中成为`Masquerade`，`Src IP`作为`Host`的`IP`成为`SNAT`。
在`iptables` 表中，你会发现`KUBE-MARK-MASQ` 表会检测出被`Masquerade`标记过的包。

![根据NodePort、LoadBalancer Service的externalTrafficPolicy的数据包路径](/images/k8s/NodePort_Policy.png)
**根据NodePort、LoadBalancer Service的externalTrafficPolicy的数据包路径图**

*图中左侧为`externalTrafficPolicy`值为`Cluster`，执行 `Masquerade`。*
`LoadBalancer Service` 的 `NodePort` 和 `externalTrafficPolicy` 的 `Cluster`。
如果`externalTrafficPolicy`值设置为`Cluster`，则请求包的`Src IP`通过`Masquerade` 被 `SNAT`到`Host`的`IP`。
在 `KUBE-NODEPORTS` 表中，可以通过 `KUBE-MARK-MASQ` 表检查所有以 `NodePort` 和 `LoadBalancer Service Port` 
作为 `Dest Port` 的数据包。

*图右侧是通过将`externalTrafficPolicy`设置为`Local`，不执行`Masquerade`*
如果将 `externalTrafficPolicy` 值设置为 `Local`，则 `KUBE-MARK-MASQ` 表中相关规则将不会出现在 `KUBE-NODEPORTS` 表中，
所以就不会执行到 `Masquerade`。请求数据包的 `Src IP` 不会被修改。另外，请求包在宿主机中不会被负载均衡，而是将请求包发送
到的 `Host` 中的目标`pod`。如果请求数据包被发送到主机不存在目标 `pod`，那么请求数据包将被丢弃。


`ExternalTrafficPolicy Local`主要用于`LoadBalancer Service`。因为由 `Cloud Provider` 的负载均衡器执行负载均衡，
所以主机不需要负载均衡，所以可以保留请求包的`Src IP`。
如果`externalTrafficPolicy`值为`Local`，云服务提供商的负载均衡器会对目标 Pod 执行健康检查，如果健康检查失败
或者目标 Pod 不存在，那么发送到主机上的数据包将会被丢弃删除。

![iptables模式下发夹前/后的数据包路径](/images/k8s/iptables_Mode_Hairpinning.png)

在`Pod`中向自己所属的`Service`的`IP`发送请求数据包，在请求数据包返回时也需要 `Masquerade`。图中左边就标识这种情况。
请求数据包被DNAT, 数据包的`Src IP`和`Dest IP`都是`Pod`本身的`IP`。
因此，`Pod`返回响应数据包时，响应数据包不通过`Host`的`NAT`表，因此不会执行`SNAT`，直接在 `Pod` 中处理。


如果使用 `Masquerade`，则可以通过将返回 `Pod` 的请求包强制传递给 Host 来执行 SNAT。这种通过故意绕过数据包来接收数据包的方法称为`Hairpinning`。
图中的右侧显示了使用`Masqurade`应用`Hairpinning`的情况。如果`KUBE-SEP-XXX`表中请求数据包的Src IP与DNAT的IP相同，
即`Pod`发送到`Service`的数据包由自己接收时，则请求的数据包经过`KUBE-MARK-MASQ` 表是被 `Marking`，
在`KUBE-POSTROUTING` 表中被`Masquerade`
因为`Pod`接收到的数据包的`Src IP`被设置为`Host`的`IP`，因此`Pod`的响应被发送到`Host`的`NAT` 表，然后进行`SNAT`和`DNAT`传递给`Pod`。

# 用户空间

Userspace Mode下的服务请求报文路径图：
![Userspace_Mode_Service_Packet_Path.png](/images/k8s/Userspace_Mode_Service_Packet_Path.png)

用户空间模式下的 KUBE-PORTALS-CONTAINER：
```bash
Chain KUBE-PORTALS-CONTAINER (1 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 REDIRECT   tcp  --  *      *       0.0.0.0/0            10.96.98.173         /* default/my-nginx-loadbalancer: */ tcp dpt:80 redir ports 38023
    0     0 REDIRECT   tcp  --  *      *       0.0.0.0/0            172.35.0.200         /* default/my-nginx-loadbalancer: */ tcp dpt:80 redir ports 38023
    0     0 REDIRECT   tcp  --  *      *       0.0.0.0/0            10.103.1.234         /* default/my-nginx-cluster: */ tcp dpt:80 redir ports 36451
    0     0 REDIRECT   tcp  --  *      *       0.0.0.0/0            10.97.229.148        /* default/my-nginx-nodeport: */ tcp dpt:80 redir ports 44257
```

用户空间模式下的 KUBE-NODEPORT-CONTAINER：
```bash
Chain KUBE-NODEPORT-CONTAINER (1 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 REDIRECT   tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/my-nginx-loadbalancer: */ tcp dpt:30781 redir ports 38023
    0     0 REDIRECT   tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/my-nginx-nodeport: */ tcp dpt:30915 redir ports 44257
```

用户空间模式下的 KUBE-PORTALS-HOST：
```bash
Chain KUBE-PORTALS-HOST (1 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            10.96.98.173         /* default/my-nginx-loadbalancer: */ tcp dpt:80 to:172.35.0.100:38023
    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            172.35.0.200         /* default/my-nginx-loadbalancer: */ tcp dpt:80 to:172.35.0.100:38023
    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            10.103.1.234         /* default/my-nginx-cluster: */ tcp dpt:80 to:172.35.0.100:46635
    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            10.97.229.148        /* default/my-nginx-nodeport: */ tcp dpt:80 to:172.35.0.100:32847
```

用户空间模式下的 KUBE-NODEPORT-HOST：
```bash
Chain KUBE-NODEPORT-HOST (1 references)
 pkts bytes target     prot opt in     out     source               destination
    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/my-nginx-loadbalancer: */ tcp dpt:30781 to:172.35.0.100:38023
    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* default/my-nginx-nodeport: */ tcp dpt:30915 to:172.35.0.100:44257
```

Service Proxy 的 iptables 模式是一种运行在用户空间的 kube-proxy 扮演 Service Proxy 角色的模式。
这是 Kubernetes 提供的第一个代理模式。目前使用的不是很好，因为与iptables模式相比性能较差。
上图中展示了 Userspace 模式下Service 请求包的路径。

由于大多数 `Pod` 传输的请求包是通过 `Pod` 的 `veth` 传递到 `Host` 的 `Network` 命名空间，因此请求包通过 PREROUTING 表传递到
`KUBE-PORTALS-CONTAINER` 表。如果 `KUBE-PORTALS-CONTAINER` 标准中请求包的 目标 `IP` 和目标 `Port` 与 `ClusterIP` 服务
的 `IP` 和 `Port` 匹配，则请求包被重定向到`kube-proxy` 。如果请求数据包的目标 `IP`是节点自己的`IP`，则将数据包投递到
`KUBE-NODEPORT-CONTAINER`表。如果 `KUBE-NODEPORT-CONTAINER` 表中请求包的 目标 `Port` 与 `NodePort Service` 的端
口匹配，则请求包被重定向到 `kube-proxy`。如果请求包的目标 `IP`和目标 `Port`与`LoadBalancer Service`的`External IP`和`Port`匹配，
请求包也会被重定向到kube-proxy。

`Pod` 或 `Host` 进程使用 `Host` 的 `Network Namespace` 传输的请求数据包由 `OUTPUT` 表 传递到 `KUBE-PORTALS-HOST` 表。
`KUBE-PORTALS-HOST` 和 `KUBE-NODEPORT-HOST` 表中的后续请求包处理类似于 `KUBE-PORTALS-CONTAINER` 和 `KUBE-NODEPORT-CONTAINER` 表
中的请求包处理。不同之处在于，DNAT是在不重定向请求包的情况下执行的。

**通过 `Redirect` 和 `DNAT` 发送到 `Service` 的所有请求数据包都被传递到 `kube-proxy`**。`kube-proxy` 收到的请求数据包的
每个目标 `Port` 映射一个服务。因此，`kube-proxy` 可以通过重定向和 `NAT` 请求数据包的 目标 `Port` 确定请求数据包应该传递到哪个服务。
`kube-proxy` 通过将接收到的请求数据包均匀地负载均衡到属于请求数据包要传输到的服务的多个 `Pod` 来重新传输接收到的请求数据包。

由于 `kube-proxy` 运行在主机的 `Network Namespace` 中，因此 `kube-proxy` 发送的请求包也会经过 `Service NAT` 表。
但是由于`kube-proxy`发送的请求包的目标 `IP`是`Pod`的`IP`，所以请求包不会被`Service NAT` 表改变，而是通过`CNI Plugin`搭建
的`Container Network`传递给`Pod`。







